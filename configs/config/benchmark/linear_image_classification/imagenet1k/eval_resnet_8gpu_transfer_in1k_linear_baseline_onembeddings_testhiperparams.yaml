# @package _global_
config:
  VERBOSE: True
  LOG_FREQUENCY: 100
  TEST_ONLY: False
  TEST_EVERY_NUM_EPOCH: 1
  TEST_MODEL: True
  SEED_VALUE: 0
  MULTI_PROCESSING_METHOD: forkserver
  HOOKS:
    PERF_STATS:
      MONITOR_PERF_STATS: False
    TENSORBOARD_SETUP:
      USE_TENSORBOARD: False
  DATA:
    NUM_DATALOADER_WORKERS: 8
    PERSISTENT_WORKERS: True
    TRAIN:
      DATA_SOURCES: [npy_dataset]
      LABEL_SOURCES: [disk_filelist]
      DATA_PATHS: ["/home/bbea/data/simclr_1node_imagenet1k_resnet50_phase999_embeddings_dataset/train_embeddings.npy"]
      LABEL_PATHS: ["/home/bbea/data/simclr_1node_imagenet1k_resnet50_phase999_embeddings_dataset/train_targets.npy"]
      BATCHSIZE_PER_REPLICA: 2048
      TRANSFORMS:
        - name: ToTensor
      MMAP_MODE: True
      COPY_TO_LOCAL_DISK: False
      COPY_DESTINATION_DIR: /tmp/imagenet1k_embeddings/
    TEST:
      DATA_SOURCES: [npy_dataset]
      LABEL_SOURCES: [disk_filelist]
      DATA_PATHS: ["/home/bbea/data/simclr_1node_imagenet1k_resnet50_phase999_embeddings_dataset/test_embeddings.npy"]
      LABEL_PATHS: ["/home/bbea/data/simclr_1node_imagenet1k_resnet50_phase999_embeddings_dataset/test_targets.npy"]
      BATCHSIZE_PER_REPLICA: 2048
      TRANSFORMS:
        - name: ToTensor
      MMAP_MODE: True
      COPY_TO_LOCAL_DISK: False
      COPY_DESTINATION_DIR: /tmp/imagenet1k_embeddings/
  METERS:
    name: accuracy_list_meter
    accuracy_list_meter:
      num_meters: 1
      topk_values: [1, 5]
  TRAINER:
    TRAIN_STEP_NAME: standard_train_step
  MODEL:
    FEATURE_EVAL_SETTINGS:
      EVAL_MODE_ON: True
      FREEZE_TRUNK_ONLY: True
      EXTRACT_TRUNK_FEATURES_ONLY: False
      SHOULD_FLATTEN_FEATS: False
    TRUNK:
      NAME: identity
    HEAD:
      PARAMS: [
        ["eval_mlp", {"in_channels": 2048, "dims": [2048, 1000]}],
      ]
    WEIGHTS_INIT:
      PARAMS_FILE: ""
      STATE_DICT_KEY_NAME: classy_state_dict
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True
      SYNC_BN_TYPE: apex
      GROUP_SIZE: 8
    AMP_PARAMS:
      USE_AMP: True
      # Only applicable for Apex AMP_TYPE.
      # Use O1 as it is robust and stable than O3. If you want to use O3, we recommend the following setting:
      # {"opt_level": "O3", "keep_batchnorm_fp32": True, "master_weights": True, "loss_scale": "dynamic"}
      AMP_ARGS: {"opt_level": "O1"}
      # we support pytorch amp as well which is availale in pytorch>=1.6.
      AMP_TYPE: "apex"  # apex | pytorch
  LOSS:
    name: cross_entropy_multiple_output_single_target
    cross_entropy_multiple_output_single_target:
      ignore_index: -1
  OPTIMIZER:
      name: sgd
      # In the OSS Caffe2 benchmark, RN50 models use 1e-4 and AlexNet models 5e-4
      weight_decay: 0.0005
      momentum: 0.9
      num_epochs: 28
      nesterov: True
      regularize_bn: False
      regularize_bias: True
      param_schedulers:
        lr:
          auto_lr_scaling:
            auto_scale: true
            base_value: 0.01
            base_lr_batch_size: 256
          name: multistep
          values: [0.01, 0.001, 0.0001, 0.00001]
          milestones: [8, 16, 24]
          update_interval: epoch
  DISTRIBUTED:
    BACKEND: nccl
    NUM_NODES: 1
    NUM_PROC_PER_NODE: 8
    INIT_METHOD: tcp
    RUN_ID: auto
  MACHINE:
    DEVICE: gpu
  CHECKPOINT:
    DIR: "."
    AUTO_RESUME: False
    CHECKPOINT_FREQUENCY: 1
